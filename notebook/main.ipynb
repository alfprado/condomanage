{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea827b8b-d775-4d00-a6d6-149e85d1867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in /opt/conda/lib/python3.11/site-packages (26.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /opt/conda/lib/python3.11/site-packages (from faker) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.11/site-packages (2.9.9)\n",
      "Collecting delta-spark\n",
      "  Obtaining dependency information for delta-spark from https://files.pythonhosted.org/packages/3c/8e/bb778f5049aa371bf80a7a781b237eb10ef104f8736fe00d25fcfee80c2b/delta_spark-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading delta_spark-3.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting pyspark<3.6.0,>=3.5.0 (from delta-spark)\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from delta-spark) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata>=1.0.0->delta-spark) (3.16.2)\n",
      "Collecting py4j==0.10.9.7 (from pyspark<3.6.0,>=3.5.0->delta-spark)\n",
      "  Obtaining dependency information for py4j==0.10.9.7 from https://files.pythonhosted.org/packages/10/30/a58b32568f1623aaad7db22aa9eafc4c6c194b429ff35bdc55ca2726da47/py4j-0.10.9.7-py2.py3-none-any.whl.metadata\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading delta_spark-3.2.0-py3-none-any.whl (21 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=228f6dc0048be243cab095928660e7fe43fbf87045be8090c99cdd466d44d295\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/95/13/41/f7f135ee114175605fb4f0a89e7389f3742aa6c1e1a5bcb657\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark, delta-spark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.4.1\n",
      "    Can't uninstall 'pyspark'. No files were found to uninstall.\n",
      "Successfully installed delta-spark-3.2.0 py4j-0.10.9.7 pyspark-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install faker\n",
    "!pip install psycopg2-binary\n",
    "!pip install delta-spark\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from faker import Faker\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "WAREHOUSE = os.environ.get(\"WAREHOUSE\") \n",
    "AWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY_ID\") \n",
    "AWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\") \n",
    "AWS_S3_ENDPOINT = os.environ.get(\"AWS_S3_ENDPOINT\") ## MINIO ENDPOINT\n",
    "PG_USER = os.environ.get(\"PG_USER\")\n",
    "PG_PASSWORD = os.environ.get(\"PG_PASSWORD\")\n",
    "PG_DB = os.environ.get(\"PG_DB\")\n",
    "\n",
    "\n",
    "def cria_conectores():\n",
    "    postgres = {\n",
    "        \"config\": {\n",
    "            \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n",
    "            \"database.dbname\": PG_DB,\n",
    "            \"database.hostname\": \"postgres\",\n",
    "            \"database.password\": PG_PASSWORD,\n",
    "            \"database.port\": \"5432\",\n",
    "            \"database.server.name\": \"postgres\",\n",
    "            \"database.user\": PG_USER,\n",
    "            \"delete.handling.mode\": \"rewrite\",\n",
    "            \"plugin.name\": \"pgoutput\",\n",
    "            \"table.include.list\": \"public.condominios, public.imoveis,public.moradores,public.transacoes\",\n",
    "            \"table.whitelist\": \"public.condominios, public.imoveis,public.moradores,public.transacoes\",\n",
    "            \"topic.prefix\": \"postgres\"\n",
    "        },\n",
    "        \"name\": \"postgres-source-connector\"\n",
    "    }\n",
    "    requests.post('http://connect:8083/connectors', json=postgres)\n",
    "    \n",
    "    minio = {\n",
    "        \"name\": \"minio-sink-connector\",\n",
    "        \"config\": {\n",
    "            \"connector.class\": \"io.aiven.kafka.connect.s3.AivenKafkaConnectS3SinkConnector\",\n",
    "            \"aws.access.key.id\": AWS_ACCESS_KEY,\n",
    "            \"aws.secret.access.key\": AWS_SECRET_KEY,\n",
    "            \"aws.s3.bucket.name\": \"condomanage\",\n",
    "            \"aws.s3.endpoint\": AWS_S3_ENDPOINT,\n",
    "            \"aws.s3.region\": \"us-east-1\",\n",
    "            \"format.output.type\": \"parquet\",\n",
    "            \"topics\": \"postgres.public.condominios, postgres.public.imoveis, postgres.public.moradores, postgres.public.transacoes\",\n",
    "            \"file.compression.type\": \"none\",\n",
    "            \"flush.size\": \"20\",\n",
    "            \"file.name.template\": \"raw/cdc/{{topic}}/{{timestamp:unit=yyyy}}{{timestamp:unit=MM}}{{timestamp:unit=dd}}_{{partition:padding=true}}-{{start_offset:padding=true}}.parquet\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    requests.post('http://connect:8083/connectors', json=minio)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Função para conectar ao banco de dados\n",
    "def connect_to_db():\n",
    "    return psycopg2.connect(\n",
    "        dbname='db',\n",
    "        user='user',\n",
    "        password='admin',\n",
    "        host='postgres',\n",
    "        port='5432'\n",
    "    )\n",
    "\n",
    "# Função para executar comandos SQL\n",
    "def execute_sql_commands(commands):\n",
    "    conn = connect_to_db()\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            for command in commands:\n",
    "                cursor.execute(command)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Inserir condomínios e recuperar IDs\n",
    "def insert_condominios():\n",
    "    conn = connect_to_db()\n",
    "    condominios = []\n",
    "    condominio_map = {}\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            for i in range(1, 101):\n",
    "                nome = fake.company()\n",
    "                endereco = fake.address().replace('\\n', ', ')\n",
    "                cursor.execute(\"INSERT INTO condominios (nome, endereco) VALUES (%s, %s) RETURNING condominio_id;\", (nome, endereco))\n",
    "                condominio_id = cursor.fetchone()[0]\n",
    "                condominio_map[condominio_id] = (nome, endereco)\n",
    "                condominios.append(condominio_id)\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while inserting condominios: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "    return condominios\n",
    "\n",
    "# Inserir moradores, imóveis e transações\n",
    "def insert_related_data(condominio_ids):\n",
    "    conn = connect_to_db()\n",
    "    moradores = []\n",
    "    imoveis = []\n",
    "    transacoes = []\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            # Inserir moradores\n",
    "            for i in range(1, 101):\n",
    "                nome = fake.name()\n",
    "                condominio_id = random.choice(condominio_ids)\n",
    "                data_registro = fake.date_this_decade()\n",
    "                cursor.execute(\"INSERT INTO moradores (nome, condominio_id, data_registro) VALUES (%s, %s, %s) RETURNING morador_id;\", (nome, condominio_id, data_registro))\n",
    "                morador_id = cursor.fetchone()[0]\n",
    "                moradores.append(morador_id)\n",
    "\n",
    "            # Inserir imóveis\n",
    "            for i in range(1, 101):\n",
    "                tipo = random.choice(['Apartamento', 'Casa'])\n",
    "                condominio_id = random.choice(condominio_ids)\n",
    "                valor = round(random.uniform(100000, 1000000), 2)\n",
    "                cursor.execute(\"INSERT INTO imoveis (tipo, condominio_id, valor) VALUES (%s, %s, %s) RETURNING imovel_id;\", (tipo, condominio_id, valor))\n",
    "                imovel_id = cursor.fetchone()[0]\n",
    "                imoveis.append(imovel_id)\n",
    "\n",
    "            # Inserir transações\n",
    "            for i in range(1, 101):\n",
    "                imovel_id = random.choice(imoveis)\n",
    "                morador_id = random.choice(moradores)\n",
    "                data_transacao = fake.date_this_year()\n",
    "                valor_transacao = round(random.uniform(50000, 500000), 2)\n",
    "                cursor.execute(\"INSERT INTO transacoes (imovel_id, morador_id, data_transacao, valor_transacao) VALUES (%s, %s, %s, %s);\", (imovel_id, morador_id, data_transacao, valor_transacao))\n",
    "\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while inserting related data: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "cria_conectores()\n",
    "# Executar o processo\n",
    "condominio_ids = insert_condominios()\n",
    "insert_related_data(condominio_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35f11dc0-5a65-4ac6-a224-bb553fe904c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INGESTÃO NO DATA LAKE\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "WAREHOUSE = os.environ.get(\"WAREHOUSE\") \n",
    "AWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY_ID\") \n",
    "AWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\") \n",
    "AWS_S3_ENDPOINT = os.environ.get(\"AWS_S3_ENDPOINT\") ## MINIO ENDPOINT\n",
    "PG_USER = os.environ.get(\"PG_USER\")\n",
    "PG_PASSWORD = os.environ.get(\"PG_PASSWORD\")\n",
    "PG_DB = os.environ.get(\"PG_DB\")\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-core_2.12:2.4.0,org.postgresql:postgresql:42.6.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\n",
    "        .set('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "        .set('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .set(\"spark.hadoop.fs.s3a..connection.ssl.enabled\", \"true\")\n",
    "        \n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint\", AWS_S3_ENDPOINT)\n",
    "        .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)\n",
    ")\n",
    "\n",
    "# Inicializa a sessão Spark com suporte ao Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataIngestion\") \\\n",
    "    .master(\"local\")\\\n",
    "    .config(conf=conf)\\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurações do PostgreSQL\n",
    "pg_url = \"jdbc:postgresql://postgres:5432/\" + PG_DB\n",
    "pg_properties = {\n",
    "    \"user\": PG_USER,\n",
    "    \"password\": PG_PASSWORD,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Carrega dados das tabelas PostgreSQL\n",
    "def load_table(table_name):\n",
    "    return spark.read.jdbc(url=pg_url, table=table_name, properties=pg_properties)\n",
    "\n",
    "# Salva os dados no MinIO em formato Parquet\n",
    "def save_to_minio(df, path):\n",
    "    df.write.mode(\"overwrite\").parquet(f\"{WAREHOUSE}/raw/full/{path}\")\n",
    "\n",
    "tables = ('condominios', 'moradores', 'transacoes', 'imoveis')\n",
    "\n",
    "for table in tables:\n",
    "    df = load_table(f\"public.{table}\")\n",
    "    save_to_minio(df, table)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c8bf409-70b3-44fb-9b45-7242c1b689f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "Criando tabela condominios...\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "Criando tabela moradores...\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "Criando tabela transacoes...\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "Criando tabela imoveis...\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BRONZE\n",
    "import os\n",
    "import pandas as pd\n",
    "import struct\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import col, expr, to_date\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, DecimalType, BinaryType\n",
    "\n",
    "\n",
    "def bytes_to_decimal(byte_list):\n",
    "    if not byte_list:\n",
    "        return None\n",
    "    \n",
    "    # Converte a lista de bytes para um número inteiro\n",
    "    integer_value = int.from_bytes(bytearray(byte_list), byteorder='big', signed=False)\n",
    "    \n",
    "    # Converte o inteiro para decimal com precisão e escala\n",
    "    precision = 15\n",
    "    scale = 2\n",
    "    decimal_value = integer_value / (10 ** scale)\n",
    "    \n",
    "    # Ajusta o valor para a precisão desejada\n",
    "    return round(decimal_value, scale)\n",
    "    \n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "WAREHOUSE = os.environ.get(\"WAREHOUSE\") \n",
    "AWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY_ID\") \n",
    "AWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\") \n",
    "AWS_S3_ENDPOINT = os.environ.get(\"AWS_S3_ENDPOINT\") ## MINIO ENDPOINT\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-core_2.12:2.4.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\n",
    "        .set('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "        .set('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint\", AWS_S3_ENDPOINT)\n",
    "        .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)\n",
    ")\n",
    "\n",
    "# Inicializa a sessão Spark com suporte ao Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataIngestion\") \\\n",
    "    .master(\"local\")\\\n",
    "    .config(conf=conf)\\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def q(query, n=30):\n",
    "    return spark.sql(query).show(n=n, truncate=False)\n",
    "\n",
    "def table_exists(namespace, table):\n",
    "    count = (spark.sql(f'SHOW TABLES FROM {namespace}')\n",
    "                .filter(f\"namespace = '{namespace}' AND tableName = '{table}'\")\n",
    "                .count())\n",
    "    return count == 1\n",
    "\n",
    "q('CREATE DATABASE IF NOT EXISTS condomanage')\n",
    "q('USE condomanage')\n",
    "\n",
    "# Carrega dados do raw\n",
    "def load_raw(table_name):\n",
    "    return spark.read.format('parquet').load(f's3a://condomanage/raw/full/{table_name}')\n",
    "    \n",
    "# Salva os dados no MinIO em formato delta\n",
    "def save_to_delta(df, path):\n",
    "    df.write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save(path)\n",
    "\n",
    "tables = ('condominios', 'moradores', 'transacoes', 'imoveis')\n",
    "namespace = 'condomanage'\n",
    "\n",
    "for table in tables:\n",
    "    if not table_exists(namespace, table):\n",
    "        print(f'Criando tabela {table}...')\n",
    "        df = load_raw(table)\n",
    "        save_to_delta(df, f\"{WAREHOUSE}/bronze/{table}\")\n",
    "        q(f\"\"\"\n",
    "            CREATE  TABLE  IF NOT EXISTS {table}\n",
    "            USING DELTA\n",
    "            LOCATION '{WAREHOUSE}/bronze/{table}'\n",
    "        \"\"\")\n",
    "    else:\n",
    "        print(f'Tabela {table} já existente...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78333c23-287c-4e7b-b93b-be72cc319531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGREGA DADOS CDC moradores\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('value', StructType([\n",
    "        StructField('before', StructType([\n",
    "            StructField('morador_id', IntegerType(), True), \n",
    "            StructField('nome', StringType(), True), \n",
    "            StructField('condominio_id', IntegerType(), True), \n",
    "            StructField('data_registro', IntegerType(), True)\n",
    "        ]), True), \n",
    "        StructField('after', StructType([\n",
    "            StructField('morador_id', IntegerType(), True), \n",
    "            StructField('nome', StringType(), True), \n",
    "            StructField('condominio_id', IntegerType(), True), \n",
    "            StructField('data_registro', IntegerType(), True)\n",
    "        ]), True), \n",
    "        StructField('source', StructType([\n",
    "            StructField('version', StringType(), True), \n",
    "            StructField('connector', StringType(), True), \n",
    "            StructField('name', StringType(), True), \n",
    "            StructField('ts_ms', LongType(), True), \n",
    "            StructField('snapshot', StringType(), True), \n",
    "            StructField('db', StringType(), True), \n",
    "            StructField('sequence', StringType(), True), \n",
    "            StructField('schema', StringType(), True), \n",
    "            StructField('table', StringType(), True), \n",
    "            StructField('txId', LongType(), True), \n",
    "            StructField('lsn', LongType(), True), \n",
    "            StructField('xmin', LongType(), True)\n",
    "        ]), True), \n",
    "        StructField('op', StringType(), True), \n",
    "        StructField('ts_ms', LongType(), True), \n",
    "        StructField('transaction', StructType([\n",
    "            StructField('id', StringType(), True), \n",
    "            StructField('total_order', LongType(), True), \n",
    "            StructField('data_collection_order', LongType(), True)\n",
    "        ]), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "df_stream = spark.readStream.format('parquet')\\\n",
    "    .schema(schema).load(f's3a://condomanage/raw/cdc/postgres.public.moradores')\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, 's3a://condomanage/bronze/moradores')\n",
    "\n",
    "stream_moradores = (df_stream.writeStream\\\n",
    "    .option(\"checkpointLocation\", f\"s3a://condomanage/raw/cdc/moradores_checkpoint/\")\\\n",
    "    .foreachBatch(lambda df, batchID: upsert_moradores(df, deltaTable))\n",
    "    .trigger(availableNow=True))\n",
    "    \n",
    "def upsert_moradores(df, deltaTable):\n",
    "    df_selecionado = df.select(\n",
    "        \"value.after.condominio_id\",\n",
    "        \"value.after.data_registro\",\n",
    "        when(df.value.op == 'd', df.value.before.morador_id).otherwise(df.value.after.morador_id).alias('morador_id'),\n",
    "        \"value.after.nome\",\n",
    "        \"value.op\",\n",
    "        \"value.before\",\n",
    "        \"value.ts_ms\"\n",
    "    )\n",
    "    \n",
    "   # RENOMEIA AS COLUNAS\n",
    "    df_selecionado = df_selecionado.withColumnRenamed(\"value.after.condominio_id\", \"condominio_id\") \\\n",
    "                                   .withColumnRenamed(\"value.after.data_registro\", \"data_registro\") \\\n",
    "                                   .withColumnRenamed(\"value.after.morador_id\", \"morador_id\") \\\n",
    "                                   .withColumnRenamed(\"value.after.nome\", \"nome\") \\\n",
    "                                   .withColumnRenamed(\"value.op\", \"op\") \\\n",
    "                                   .withColumnRenamed(\"value.before\", \"before\") \\\n",
    "                                   .withColumnRenamed(\"value.ts_ms\", \"ts_ms\")\n",
    "    \n",
    "    # SELECIONA A ULTIMA ATUALIZAÇÃO DO DADO\n",
    "    df_selecionado.createOrReplaceGlobalTempView('view_moradores')\n",
    "    \n",
    "    cdc_unique = spark.sql('''\n",
    "    WITH \n",
    "        qualify as (select \n",
    "            *, \n",
    "            ROW_NUMBER() over(partition by morador_id order by ts_ms desc) as qualify \n",
    "        from global_temp.view_moradores)\n",
    "    select * from qualify where qualify = 1''')\n",
    "    \n",
    "    # AJUSTA O TIPO DA COLUNA DATA_REGISTRO\n",
    "    cdc_unique = cdc_unique.withColumn(\"data_registro\", to_date(expr(\"date_add('1970-01-01', data_registro)\")))\n",
    "    \n",
    "    # UPSERT\n",
    "    (deltaTable.alias('b')\n",
    "        .merge(cdc_unique.alias('d'), \n",
    "        'b.morador_id = d.morador_id')\n",
    "        .whenMatchedUpdateAll(condition = \"d.op = 'u'\")\n",
    "        .whenMatchedDelete(condition = \"d.op = 'd'\")\n",
    "        .whenNotMatchedInsertAll(condition = \"d.op = 'c' or d.op = 'u'\")\n",
    "        .execute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78480d8-a9b4-4349-8a2d-1a3ddfdc109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGREGA CDC CONDOMINIOS\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('value', StructType([\n",
    "        StructField('before', StructType([\n",
    "            StructField('condominio_id', IntegerType(), True), \n",
    "            StructField('nome', StringType(), True), \n",
    "            StructField('endereco', StringType(), True)\n",
    "        ]), True), \n",
    "        StructField('after', \n",
    "            StructType([\n",
    "                StructField('condominio_id', IntegerType(), True), \n",
    "                StructField('nome', StringType(), True), \n",
    "                StructField('endereco', StringType(), True)\n",
    "            ]), True), \n",
    "        StructField('source', \n",
    "                    StructType([\n",
    "                        StructField('version', StringType(), True), \n",
    "                        StructField('connector', StringType(), True), \n",
    "                        StructField('name', StringType(), True), \n",
    "                        StructField('ts_ms', LongType(), True), \n",
    "                        StructField('snapshot', StringType(), True),\n",
    "                        StructField('db', StringType(), True), \n",
    "                        StructField('sequence', StringType(), True), \n",
    "                        StructField('schema', StringType(), True), \n",
    "                        StructField('table', StringType(), True), \n",
    "                        StructField('txId', LongType(), True), \n",
    "                        StructField('lsn', LongType(), True), \n",
    "                        StructField('xmin', LongType(), True)\n",
    "                    ]), True), \n",
    "        StructField('op', StringType(), True), \n",
    "        StructField('ts_ms', LongType(), True), \n",
    "        StructField('transaction', StructType([\n",
    "            StructField('id', StringType(), True), \n",
    "            StructField('total_order', LongType(), True), \n",
    "            StructField('data_collection_order', LongType(), True)\n",
    "        ]), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "df_stream = spark.readStream.format('parquet')\\\n",
    "    .schema(schema).load(f's3a://condomanage/raw/cdc/postgres.public.condominios')\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, 's3a://condomanage/bronze/condominios')\n",
    "\n",
    "stream_condominios = (df_stream.writeStream\\\n",
    "    .option(\"checkpointLocation\", f\"s3a://condomanage/raw/cdc/condominios_checkpoint/\")\\\n",
    "    .foreachBatch(lambda df, batchID: upsert_condominios(df, deltaTable))\n",
    "    .trigger(availableNow=True))\n",
    "    \n",
    "\n",
    "\n",
    "def upsert_condominios(df, deltaTable):\n",
    "    df_selecionado = df.select(\n",
    "        \"value.after.endereco\",\n",
    "        \"value.after.nome\",\n",
    "        when(df.value.op == 'd', df.value.before.condominio_id).otherwise(df.value.after.condominio_id).alias('condominio_id'),\n",
    "        \"value.op\",\n",
    "        \"value.before\",\n",
    "        \"value.ts_ms\"\n",
    "    )\n",
    "    \n",
    "   # RENOMEIA AS COLUNAS\n",
    "    df_selecionado = df_selecionado.withColumnRenamed(\"value.after.endereco\", \"endereco\") \\\n",
    "                                   .withColumnRenamed(\"value.after.nome\", \"nome\") \\\n",
    "                                   .withColumnRenamed(\"value.after.condominio_id\", \"condominio_id\") \\\n",
    "                                   .withColumnRenamed(\"value.op\", \"op\") \\\n",
    "                                   .withColumnRenamed(\"value.before\", \"before\") \\\n",
    "                                   .withColumnRenamed(\"value.ts_ms\", \"ts_ms\")\n",
    "    \n",
    "    # SELECIONA A ULTIMA ATUALIZAÇÃO DO DADO\n",
    "    df_selecionado.createOrReplaceGlobalTempView('view_condominios')\n",
    "    \n",
    "    cdc_unique = spark.sql('''\n",
    "    WITH \n",
    "        qualify as (select \n",
    "            *, \n",
    "            ROW_NUMBER() over(partition by condominio_id order by ts_ms desc) as qualify \n",
    "        from global_temp.view_condominios)\n",
    "    select * from qualify where qualify = 1''')\n",
    "    \n",
    "    # UPSERT\n",
    "    (deltaTable.alias('b')\n",
    "        .merge(cdc_unique.alias('d'), \n",
    "        'b.condominio_id = d.condominio_id')\n",
    "        .whenMatchedUpdateAll(condition = \"d.op = 'u'\")\n",
    "        .whenMatchedDelete(condition = \"d.op = 'd'\")\n",
    "        .whenNotMatchedInsertAll(condition = \"d.op = 'c'\")\n",
    "        .execute()\n",
    "    )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad051139-9ac7-4511-8cf8-49ec56fbf8af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IMOVEIS Stream\n",
    "# Definindo o schema do PySpark\n",
    "schema = StructType([\n",
    "StructField('value', StructType([\n",
    "    StructField(\"before\", StructType([\n",
    "        StructField(\"imovel_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"tipo\", StringType(), nullable=False),\n",
    "        StructField(\"condominio_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"valor\", BinaryType(), nullable=False)  # Coluna binária para decimal\n",
    "    ]), nullable=True),\n",
    "    StructField(\"after\", StructType([\n",
    "        StructField(\"imovel_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"tipo\", StringType(), nullable=False),\n",
    "        StructField(\"condominio_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"valor\", BinaryType(), nullable=False)  # Coluna binária para decimal\n",
    "    ]), nullable=True),\n",
    "    StructField(\"source\", StructType([\n",
    "        StructField(\"version\", StringType(), nullable=False),\n",
    "        StructField(\"connector\", StringType(), nullable=False),\n",
    "        StructField(\"name\", StringType(), nullable=False),\n",
    "        StructField(\"ts_ms\", LongType(), nullable=False),\n",
    "        StructField(\"snapshot\", StringType(), nullable=True),\n",
    "        StructField(\"db\", StringType(), nullable=False),\n",
    "        StructField(\"sequence\", StringType(), nullable=True),\n",
    "        StructField(\"schema\", StringType(), nullable=False),\n",
    "        StructField(\"table\", StringType(), nullable=False),\n",
    "        StructField(\"txId\", LongType(), nullable=True),\n",
    "        StructField(\"lsn\", LongType(), nullable=True),\n",
    "        StructField(\"xmin\", LongType(), nullable=True)\n",
    "    ]), nullable=False),\n",
    "    StructField(\"op\", StringType(), nullable=False),\n",
    "    StructField(\"ts_ms\", LongType(), nullable=True),\n",
    "    StructField(\"transaction\", StructType([\n",
    "        StructField(\"id\", StringType(), nullable=False),\n",
    "        StructField(\"total_order\", LongType(), nullable=False),\n",
    "        StructField(\"data_collection_order\", LongType(), nullable=False)\n",
    "    ]), nullable=True)\n",
    "]))\n",
    "])\n",
    "\n",
    "df_stream = spark.readStream.format('parquet')\\\n",
    "    .schema(schema).load(f's3a://condomanage/raw/cdc/postgres.public.imoveis')\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, 's3a://condomanage/bronze/imoveis')\n",
    "\n",
    "stream_imoveis = (df_stream.writeStream\\\n",
    "    .option(\"checkpointLocation\", f\"s3a://condomanage/raw/cdc/imoveis_checkpoint/\")\\\n",
    "    .foreachBatch(lambda df, batchID: upsert_imoveis(df, deltaTable))\n",
    "    .trigger(availableNow=True))\n",
    "    \n",
    "\n",
    "\n",
    "def upsert_imoveis(df, deltaTable):\n",
    "    df_selecionado = df.select(\n",
    "        \"value.after.tipo\",\n",
    "        \"value.after.valor\",\n",
    "        \"value.after.condominio_id\",\n",
    "        when(df.value.op == 'd', df.value.before.imovel_id).otherwise(df.value.after.imovel_id).alias('imovel_id'),\n",
    "        \"value.op\",\n",
    "        #\"value.before\",\n",
    "        \"value.ts_ms\"\n",
    "    )\n",
    "    \n",
    "    df_selecionado = df_selecionado.withColumnRenamed(\"value.after.tipo\", \"tipo\") \\\n",
    "                                   .withColumnRenamed(\"value.after.valor\", \"valor\") \\\n",
    "                                   .withColumnRenamed(\"value.after.condominio_id\", \"condominio_id\") \\\n",
    "                                    .withColumnRenamed(\"value.after.imovel_id\", \"imovel_id\") \\\n",
    "                                   .withColumnRenamed(\"value.op\", \"op\") \\\n",
    "                                   .withColumnRenamed(\"value.ts_ms\", \"ts_ms\")\n",
    "    df =df_selecionado.toPandas()\n",
    "    \n",
    "    df['valor'] = df.apply(lambda x: bytes_to_decimal(x['valor']), axis=1) \n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"tipo\", StringType(), nullable=False),\n",
    "        StructField(\"valor\", DecimalType(15, 2), nullable=False),\n",
    "        StructField(\"condominio_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"imovel_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"condominio_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"op\", StringType(), nullable=False),\n",
    "        StructField(\"ts_ms\", LongType(), nullable=True)\n",
    "    ])\n",
    "    \n",
    "    df_selecionado = spark.createDataFrame(df)\n",
    "    \n",
    "    # SELECIONA A ULTIMA ATUALIZAÇÃO DO DADO\n",
    "    df_selecionado.createOrReplaceGlobalTempView('view_imoveis')\n",
    "    \n",
    "    cdc_unique = spark.sql('''\n",
    "    WITH \n",
    "        qualify as (select \n",
    "            *, \n",
    "            ROW_NUMBER() over(partition by imovel_id order by ts_ms desc) as qualify \n",
    "        from global_temp.view_imoveis)\n",
    "    select * from qualify where qualify = 1''')\n",
    "    \n",
    "    # UPSERT\n",
    "    (deltaTable.alias('b')\n",
    "        .merge(cdc_unique.alias('d'), \n",
    "        'b.imovel_id = d.imovel_id')\n",
    "        .whenMatchedUpdateAll(condition = \"d.op = 'u'\")\n",
    "        .whenMatchedDelete(condition = \"d.op = 'd'\")\n",
    "        .whenNotMatchedInsertAll(condition = \"d.op = 'c'\")\n",
    "        .execute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29b7ed73-6199-4743-87bf-d87202cf3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGREGA CDC TRANSACOES\n",
    "\n",
    "# Definindo o schema do PySpark\n",
    "schema = StructType([\n",
    "StructField('value', StructType([\n",
    "    StructField(\"before\", StructType([\n",
    "        StructField(\"transacao_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"imovel_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"morador_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"data_transacao\", IntegerType(), nullable=False),\n",
    "        StructField(\"valor_transacao\", BinaryType(), nullable=False),# Coluna binária para decimal\n",
    "    ]), nullable=True),\n",
    "    StructField(\"after\", StructType([\n",
    "        StructField(\"transacao_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"imovel_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"morador_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"data_transacao\", IntegerType(), nullable=False),\n",
    "        StructField(\"valor_transacao\", BinaryType(), nullable=False),# Coluna binária para decimal\n",
    "    ]), nullable=True),\n",
    "    StructField(\"source\", StructType([\n",
    "        StructField(\"version\", StringType(), nullable=False),\n",
    "        StructField(\"connector\", StringType(), nullable=False),\n",
    "        StructField(\"name\", StringType(), nullable=False),\n",
    "        StructField(\"ts_ms\", LongType(), nullable=False),\n",
    "        StructField(\"snapshot\", StringType(), nullable=True),\n",
    "        StructField(\"db\", StringType(), nullable=False),\n",
    "        StructField(\"sequence\", StringType(), nullable=True),\n",
    "        StructField(\"schema\", StringType(), nullable=False),\n",
    "        StructField(\"table\", StringType(), nullable=False),\n",
    "        StructField(\"txId\", LongType(), nullable=True),\n",
    "        StructField(\"lsn\", LongType(), nullable=True),\n",
    "        StructField(\"xmin\", LongType(), nullable=True)\n",
    "    ]), nullable=False),\n",
    "    StructField(\"op\", StringType(), nullable=False),\n",
    "    StructField(\"ts_ms\", LongType(), nullable=True),\n",
    "    StructField(\"transaction\", StructType([\n",
    "        StructField(\"id\", StringType(), nullable=False),\n",
    "        StructField(\"total_order\", LongType(), nullable=False),\n",
    "        StructField(\"data_collection_order\", LongType(), nullable=False)\n",
    "    ]), nullable=True)\n",
    "]))\n",
    "])\n",
    "\n",
    "df_stream = spark.readStream.format('parquet')\\\n",
    "    .schema(schema).load(f's3a://condomanage/raw/cdc/postgres.public.transacoes')\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, 's3a://condomanage/bronze/transacoes')\n",
    "\n",
    "stream_transacoes = (df_stream.writeStream\\\n",
    "    .option(\"checkpointLocation\", f\"s3a://condomanage/raw/cdc/transacoes_checkpoint/\")\\\n",
    "    .foreachBatch(lambda df, batchID: upsert_transacoes(df, deltaTable))\n",
    "    .trigger(availableNow=True))\n",
    "    \n",
    "\n",
    "\n",
    "def upsert_transacoes(df, deltaTable):\n",
    "    df_selecionado = df.select(\n",
    "        \"value.after.imovel_id\",\n",
    "        \"value.after.morador_id\",\n",
    "        \"value.after.data_transacao\",\n",
    "        \"value.after.valor_transacao\",\n",
    "        when(df.value.op == 'd', df.value.before.transacao_id).otherwise(df.value.after.transacao_id).alias('transacao_id'),\n",
    "        \"value.op\",\n",
    "        \"value.ts_ms\"\n",
    "    )\n",
    "    \n",
    "    df_selecionado = df_selecionado.withColumnRenamed(\"value.after.imovel_id\", \"imovel_id\") \\\n",
    "                                   .withColumnRenamed(\"value.after.morador_id\", \"morador_id\") \\\n",
    "                                   .withColumnRenamed(\"value.after.data_transacao\", \"data_transacao\") \\\n",
    "                                   .withColumnRenamed(\"value.after.valor_transacao\", \"valor_transacao\") \\\n",
    "                                   .withColumnRenamed(\"value.after.transacao_id\", \"transacao_id\") \\\n",
    "                                   .withColumnRenamed(\"value.op\", \"op\") \\\n",
    "                                   .withColumnRenamed(\"value.ts_ms\", \"ts_ms\")\n",
    "    df = df_selecionado.toPandas()\n",
    "\n",
    "    df['valor_transacao'] = df.apply(lambda x: bytes_to_decimal(x['valor_transacao']), axis=1) \n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"imovel_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"morador_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"data_transacao\", IntegerType(), nullable=False),\n",
    "        StructField(\"valor_transacao\", DecimalType(15, 2), nullable=False),\n",
    "        StructField(\"transacao_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"op\", StringType(), nullable=False),\n",
    "        StructField(\"ts_ms\", LongType(), nullable=True)\n",
    "    ])\n",
    "    \n",
    "    df_selecionado = spark.createDataFrame(df)\n",
    "    \n",
    "    # SELECIONA A ULTIMA ATUALIZAÇÃO DO DADO\n",
    "    df_selecionado.createOrReplaceGlobalTempView('view_transacoes')\n",
    "    \n",
    "    cdc_unique = spark.sql('''\n",
    "    WITH \n",
    "        qualify as (select \n",
    "            imovel_id, morador_id, cast(data_transacao as integer) as data_transacao, valor_transacao, transacao_id, op, ts_ms, \n",
    "            ROW_NUMBER() over(partition by transacao_id order by ts_ms desc) as qualify \n",
    "        from global_temp.view_transacoes)\n",
    "    select * from qualify where qualify = 1''')\n",
    "\n",
    "    # AJUSTA O TIPO DA COLUNA DATA_REGISTRO\n",
    "    cdc_unique = cdc_unique.withColumn(\"data_transacao\", to_date(expr(\"date_add('1970-01-01', data_transacao)\")))\n",
    "    \n",
    "    # UPSERT\n",
    "    (deltaTable.alias('b')\n",
    "        .merge(cdc_unique.alias('d'), \n",
    "        'b.transacao_id = d.transacao_id')\n",
    "        .whenMatchedUpdateAll(condition = \"d.op = 'u'\")\n",
    "        .whenMatchedDelete(condition = \"d.op = 'd'\")\n",
    "        .whenNotMatchedInsertAll(condition = \"d.op = 'c' or d.op = 'u'\")\n",
    "        .execute()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02b9a83c-6cac-48d8-9037-7be0a30bad61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7aec75d90250>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_transacoes.start()\n",
    "stream_imoveis.start()\n",
    "stream_moradores.start()\n",
    "stream_condominios.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d480d2d-3fa3-438e-9197-dcc7a45f89a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 115, in call\n",
      "    raise e\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 112, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_120/3964586337.py\", line 48, in <lambda>\n",
      "    .foreachBatch(lambda df, batchID: upsert_condominios(df, deltaTable))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_120/3964586337.py\", line 89, in upsert_condominios\n",
      "    .execute()\n",
      "     ^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/delta/tables.py\", line 1036, in execute\n",
      "    self._jbuilder.execute()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Cannot resolve b.condominio_id in search condition given columns b.transacao_id, b.imovel_id, b.morador_id, b.data_transacao, b.valor_transacao, d.endereco, d.nome, d.condominio_id, d.op, d.before, d.ts_ms, d.qualify; line 1 pos 0\n",
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 115, in call\n",
      "    raise e\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 112, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_120/1110974146.py\", line 48, in <lambda>\n",
      "    .foreachBatch(lambda df, batchID: upsert_moradores(df, deltaTable))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_120/1110974146.py\", line 92, in upsert_moradores\n",
      "    .execute()\n",
      "     ^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/delta/tables.py\", line 1036, in execute\n",
      "    self._jbuilder.execute()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Cannot resolve transacao_id in UPDATE clause given columns d.condominio_id, d.data_registro, d.morador_id, d.nome, d.op, d.before, d.ts_ms, d.qualify\n",
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 115, in call\n",
      "    raise e\n",
      "  File \"/usr/local/spark/python/pyspark/sql/utils.py\", line 112, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_120/3456838899.py\", line 48, in <lambda>\n",
      "    .foreachBatch(lambda df, batchID: upsert_imoveis(df, deltaTable))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_120/3456838899.py\", line 104, in upsert_imoveis\n",
      "    .execute()\n",
      "     ^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/delta/tables.py\", line 1036, in execute\n",
      "    self._jbuilder.execute()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Cannot resolve transacao_id in UPDATE clause given columns d.tipo, d.valor, d.condominio_id, d.imovel_id, d.op, d.ts_ms, d.qualify\n"
     ]
    }
   ],
   "source": [
    "# Calcular total de transacoes\n",
    "spark.sql('''\n",
    "select c.nome, sum(t.valor_transacao) as total from transacoes t \n",
    "    inner join imoveis i on i.imovel_id = t.imovel_id\n",
    "    inner join condominios c on c.condominio_id = i.condominio_id\n",
    "group by c.condominio_id, c.nome order by 2 desc''').write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save('s3a://condomanage/gold/total_transacoes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad13a59e-48ed-483a-8757-fdabbd881a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular valor de transacoes por morador\n",
    "spark.sql('''\n",
    "select \n",
    "    m.nome, \n",
    "    sum(t.valor_transacao) as total \n",
    "from transacoes t \n",
    "    inner join moradores m on t.morador_id = m.morador_id\n",
    "group by m.nome order by 2 desc''').write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save('s3a://condomanage/gold/transacoes_morador')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d4ea4cf-ab39-4e46-a78b-d5a8de4c7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar as transações diárias por tipo de imóvel\n",
    "spark.sql('''\n",
    "SELECT \n",
    "    t.data_transacao,\n",
    "    i.tipo,\n",
    "    SUM(t.valor_transacao) AS total_valor_transacao\n",
    "FROM \n",
    "    transacoes t\n",
    "JOIN \n",
    "    imoveis i ON t.imovel_id = i.imovel_id\n",
    "GROUP BY \n",
    "    t.data_transacao,\n",
    "    i.tipo\n",
    "ORDER BY \n",
    "    t.data_transacao,\n",
    "    i.tipo;''').write.format('delta').mode('overwrite').option(\"mergeSchema\", \"true\").save('s3a://condomanage/gold/transacoes_imovel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584b9998-26d6-48ab-b32e-c1ad049a05a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
