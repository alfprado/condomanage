{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea827b8b-d775-4d00-a6d6-149e85d1867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in /opt/conda/lib/python3.11/site-packages (26.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /opt/conda/lib/python3.11/site-packages (from faker) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.11/site-packages (2.9.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install faker\n",
    "!pip install psycopg2-binary\n",
    "import requests\n",
    "import json\n",
    "from faker import Faker\n",
    "import random\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from faker import Faker\n",
    "import random\n",
    "import psycopg2\n",
    "\n",
    "def cria_conectores():\n",
    "    postgres = {\n",
    "        \"config\": {\n",
    "            \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n",
    "            \"database.dbname\": \"db\",\n",
    "            \"database.hostname\": \"postgres\",\n",
    "            \"database.password\": \"admin\",\n",
    "            \"database.port\": \"5432\",\n",
    "            \"database.server.name\": \"postgres\",\n",
    "            \"database.user\": \"user\",\n",
    "            \"delete.handling.mode\": \"rewrite\",\n",
    "            \"plugin.name\": \"pgoutput\",\n",
    "            \"table.include.list\": \"public.condominios, public.imoveis,public.moradores,public.transacoes\",\n",
    "            \"table.whitelist\": \"public.condominios, public.imoveis,public.moradores,public.transacoes\",\n",
    "            \"topic.prefix\": \"postgres\"\n",
    "        },\n",
    "        \"name\": \"postgres-source-connector\"\n",
    "    }\n",
    "    requests.post('http://connect:8083/connectors', json=postgres)\n",
    "    \n",
    "    minio = {\n",
    "        \"name\": \"minio-sink-connector\",\n",
    "        \"config\": {\n",
    "            \"connector.class\": \"io.aiven.kafka.connect.s3.AivenKafkaConnectS3SinkConnector\",\n",
    "            \"aws.access.key.id\": \"SmRXzZ0KrpROvPSKDddy\",\n",
    "            \"aws.secret.access.key\": \"ApanXjLP51CxV3bjPqOlankgLDpeoIZyR4xewaOS\",\n",
    "            \"aws.s3.bucket.name\": \"condomanage\",\n",
    "            \"aws.s3.endpoint\": \"http://minio:9000\",\n",
    "            \"aws.s3.region\": \"us-east-1\",\n",
    "            \"format.output.type\": \"parquet\",\n",
    "            \"topics\": \"postgres.public.condominios, postgres.public.imoveis, postgres.public.moradores, postgres.public.transacoes\",\n",
    "            \"file.compression.type\": \"none\",\n",
    "            \"flush.size\": \"20\",\n",
    "            \"file.name.template\": \"raw/cdc/{{topic}}/{{timestamp:unit=yyyy}}{{timestamp:unit=MM}}{{timestamp:unit=dd}}_{{partition:padding=true}}-{{start_offset:padding=true}}.parquet\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    requests.post('http://connect:8083/connectors', json=minio)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Função para conectar ao banco de dados\n",
    "def connect_to_db():\n",
    "    return psycopg2.connect(\n",
    "        dbname='db',\n",
    "        user='user',\n",
    "        password='admin',\n",
    "        host='postgres',\n",
    "        port='5432'\n",
    "    )\n",
    "\n",
    "# Função para executar comandos SQL\n",
    "def execute_sql_commands(commands):\n",
    "    conn = connect_to_db()\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            for command in commands:\n",
    "                cursor.execute(command)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Inserir condomínios e recuperar IDs\n",
    "def insert_condominios():\n",
    "    conn = connect_to_db()\n",
    "    condominios = []\n",
    "    condominio_map = {}\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            for i in range(1, 101):\n",
    "                nome = fake.company()\n",
    "                endereco = fake.address().replace('\\n', ', ')\n",
    "                cursor.execute(\"INSERT INTO condominios (nome, endereco) VALUES (%s, %s) RETURNING condominio_id;\", (nome, endereco))\n",
    "                condominio_id = cursor.fetchone()[0]\n",
    "                condominio_map[condominio_id] = (nome, endereco)\n",
    "                condominios.append(condominio_id)\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while inserting condominios: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "    return condominios\n",
    "\n",
    "# Inserir moradores, imóveis e transações\n",
    "def insert_related_data(condominio_ids):\n",
    "    conn = connect_to_db()\n",
    "    moradores = []\n",
    "    imoveis = []\n",
    "    transacoes = []\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            # Inserir moradores\n",
    "            for i in range(1, 101):\n",
    "                nome = fake.name()\n",
    "                condominio_id = random.choice(condominio_ids)\n",
    "                data_registro = fake.date_this_decade()\n",
    "                cursor.execute(\"INSERT INTO moradores (nome, condominio_id, data_registro) VALUES (%s, %s, %s) RETURNING morador_id;\", (nome, condominio_id, data_registro))\n",
    "                morador_id = cursor.fetchone()[0]\n",
    "                moradores.append(morador_id)\n",
    "\n",
    "            # Inserir imóveis\n",
    "            for i in range(1, 101):\n",
    "                tipo = random.choice(['Apartamento', 'Casa'])\n",
    "                condominio_id = random.choice(condominio_ids)\n",
    "                valor = round(random.uniform(100000, 1000000), 2)\n",
    "                cursor.execute(\"INSERT INTO imoveis (tipo, condominio_id, valor) VALUES (%s, %s, %s) RETURNING imovel_id;\", (tipo, condominio_id, valor))\n",
    "                imovel_id = cursor.fetchone()[0]\n",
    "                imoveis.append(imovel_id)\n",
    "\n",
    "            # Inserir transações\n",
    "            for i in range(1, 101):\n",
    "                imovel_id = random.choice(imoveis)\n",
    "                morador_id = random.choice(moradores)\n",
    "                data_transacao = fake.date_this_year()\n",
    "                valor_transacao = round(random.uniform(50000, 500000), 2)\n",
    "                cursor.execute(\"INSERT INTO transacoes (imovel_id, morador_id, data_transacao, valor_transacao) VALUES (%s, %s, %s, %s);\", (imovel_id, morador_id, data_transacao, valor_transacao))\n",
    "\n",
    "            conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while inserting related data: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Executar o processo\n",
    "condominio_ids = insert_condominios()\n",
    "insert_related_data(condominio_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f11dc0-5a65-4ac6-a224-bb553fe904c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INGESTÃO NO DATA LAKE\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "WAREHOUSE = os.environ.get(\"WAREHOUSE\") \n",
    "AWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY_ID\") \n",
    "AWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\") \n",
    "AWS_S3_ENDPOINT = os.environ.get(\"AWS_S3_ENDPOINT\") ## MINIO ENDPOINT\n",
    "PG_USER = os.environ.get(\"PG_USER\")\n",
    "PG_PASSWORD = os.environ.get(\"PG_PASSWORD\")\n",
    "PG_DB = os.environ.get(\"PG_DB\")\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-core_2.12:2.4.0,org.postgresql:postgresql:42.6.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\n",
    "        .set('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "        .set('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .set(\"spark.hadoop.fs.s3a..connection.ssl.enabled\", \"true\")\n",
    "        \n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint\", AWS_S3_ENDPOINT)\n",
    "        .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)\n",
    ")\n",
    "\n",
    "# Inicializa a sessão Spark com suporte ao Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataIngestion\") \\\n",
    "    .master(\"local\")\\\n",
    "    .config(conf=conf)\\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurações do PostgreSQL\n",
    "pg_url = \"jdbc:postgresql://postgres:5432/\" + PG_DB\n",
    "pg_properties = {\n",
    "    \"user\": PG_USER,\n",
    "    \"password\": PG_PASSWORD,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Carrega dados das tabelas PostgreSQL\n",
    "def load_table(table_name):\n",
    "    return spark.read.jdbc(url=pg_url, table=table_name, properties=pg_properties)\n",
    "\n",
    "# Salva os dados no MinIO em formato Parquet\n",
    "def save_to_minio(df, path):\n",
    "    df.write.mode(\"overwrite\").parquet(f\"{WAREHOUSE}/raw/full/{path}\")\n",
    "\n",
    "tables = ('condominios', 'moradores', 'transacoes', 'imoveis')\n",
    "\n",
    "for table in tables:\n",
    "    df = load_table(f\"public.{table}\")\n",
    "    save_to_minio(df, table)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c8bf409-70b3-44fb-9b45-7242c1b689f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "Criando tabela condominios...\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "Criando tabela moradores...\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "Criando tabela transacoes...\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "Criando tabela imoveis...\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BRONZE\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import col, expr, to_date\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "import pandas as pd\n",
    "import struct\n",
    "\n",
    "def bytes_to_decimal(byte_list):\n",
    "    if not byte_list:\n",
    "        return None\n",
    "    \n",
    "    # Converte a lista de bytes para um número inteiro\n",
    "    integer_value = int.from_bytes(bytearray(byte_list), byteorder='big', signed=False)\n",
    "    \n",
    "    # Converte o inteiro para decimal com precisão e escala\n",
    "    precision = 15\n",
    "    scale = 2\n",
    "    decimal_value = integer_value / (10 ** scale)\n",
    "    \n",
    "    # Ajusta o valor para a precisão desejada\n",
    "    return round(decimal_value, scale)\n",
    "    \n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "WAREHOUSE = os.environ.get(\"WAREHOUSE\") \n",
    "AWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY_ID\") \n",
    "AWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\") \n",
    "AWS_S3_ENDPOINT = os.environ.get(\"AWS_S3_ENDPOINT\") ## MINIO ENDPOINT\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-core_2.12:2.4.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\n",
    "        .set('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "        .set('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minioserver:9000\")\n",
    "        .set('spark.hadoop.fs.s3a.access.key', 'SmRXzZ0KrpROvPSKDddy')\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', 'ApanXjLP51CxV3bjPqOlankgLDpeoIZyR4xewaOS')\n",
    ")\n",
    "\n",
    "# Inicializa a sessão Spark com suporte ao Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataIngestion\") \\\n",
    "    .master(\"local\")\\\n",
    "    .config(conf=conf)\\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def q(query, n=30):\n",
    "    return spark.sql(query).show(n=n, truncate=False)\n",
    "\n",
    "def table_exists(namespace, table):\n",
    "    count = (spark.sql(f'SHOW TABLES FROM {namespace}')\n",
    "                .filter(f\"namespace = '{namespace}' AND tableName = '{table}'\")\n",
    "                .count())\n",
    "    return count == 1\n",
    "\n",
    "q('CREATE DATABASE IF NOT EXISTS condomanage')\n",
    "q('USE condomanage')\n",
    "\n",
    "# Carrega dados do raw\n",
    "def load_raw(table_name):\n",
    "    return spark.read.format('parquet').load(f's3a://condomanage/raw/full/{table_name}')\n",
    "    \n",
    "# Salva os dados no MinIO em formato delta\n",
    "def save_to_delta(df, path):\n",
    "    df.write.format('delta').mode('overwrite').save(path)\n",
    "\n",
    "tables = ('condominios', 'moradores', 'transacoes', 'imoveis')\n",
    "namespace = 'condomanage'\n",
    "\n",
    "for table in tables:\n",
    "    if table_exists(namespace, table):\n",
    "        print(f'Criando tabela {table}...')\n",
    "        q(f\"\"\"\n",
    "            CREATE  TABLE  IF NOT EXISTS {table}\n",
    "            USING DELTA\n",
    "            LOCATION '{WAREHOUSE}/bronze/{table}'\n",
    "        \"\"\")\n",
    "        df = load_raw(table)\n",
    "        save_to_delta(df, f\"{WAREHOUSE}/bronze/{path}\")\n",
    "    else:\n",
    "        print(f'Tabela {table} já existente...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "718ca300-d14a-4222-bd2f-fe62107a883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGREGA DADOS CDC moradores\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('value', StructType([\n",
    "        StructField('before', StructType([\n",
    "            StructField('morador_id', IntegerType(), True), \n",
    "            StructField('nome', StringType(), True), \n",
    "            StructField('condominio_id', IntegerType(), True), \n",
    "            StructField('data_registro', IntegerType(), True)\n",
    "        ]), True), \n",
    "        StructField('after', StructType([\n",
    "            StructField('morador_id', IntegerType(), True), \n",
    "            StructField('nome', StringType(), True), \n",
    "            StructField('condominio_id', IntegerType(), True), \n",
    "            StructField('data_registro', IntegerType(), True)\n",
    "        ]), True), \n",
    "        StructField('source', StructType([\n",
    "            StructField('version', StringType(), True), \n",
    "            StructField('connector', StringType(), True), \n",
    "            StructField('name', StringType(), True), \n",
    "            StructField('ts_ms', LongType(), True), \n",
    "            StructField('snapshot', StringType(), True), \n",
    "            StructField('db', StringType(), True), \n",
    "            StructField('sequence', StringType(), True), \n",
    "            StructField('schema', StringType(), True), \n",
    "            StructField('table', StringType(), True), \n",
    "            StructField('txId', LongType(), True), \n",
    "            StructField('lsn', LongType(), True), \n",
    "            StructField('xmin', LongType(), True)\n",
    "        ]), True), \n",
    "        StructField('op', StringType(), True), \n",
    "        StructField('ts_ms', LongType(), True), \n",
    "        StructField('transaction', StructType([\n",
    "            StructField('id', StringType(), True), \n",
    "            StructField('total_order', LongType(), True), \n",
    "            StructField('data_collection_order', LongType(), True)\n",
    "        ]), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "df_stream = spark.readStream.format('parquet')\\\n",
    "    .schema(schema).load(f's3a://condomanage/raw/cdc/postgres.public.moradores')\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, 's3a://condomanage/bronze/moradores')\n",
    "\n",
    "stream_moradores = (df_stream.writeStream\\\n",
    "    .option(\"checkpointLocation\", f\"s3a://condomanage/raw/cdc/moradores_checkpoint/\")\\\n",
    "    .foreachBatch(lambda df, batchID: upsert_moradores(df, deltaTable))\n",
    "    .trigger(availableNow=True))\n",
    "    \n",
    "\n",
    "\n",
    "def upsert_moradores(df, deltaTable):\n",
    "    df_selecionado = df.select(\n",
    "        \"value.after.condominio_id\",\n",
    "        \"value.after.data_registro\",\n",
    "        when(df.value.op == 'd', df.value.before.morador_id).otherwise(df.value.after.morador_id).alias('morador_id'),\n",
    "        \"value.after.nome\",\n",
    "        \"value.op\",\n",
    "        \"value.before\",\n",
    "        \"value.ts_ms\"\n",
    "    )\n",
    "    \n",
    "   # RENOMEIA AS COLUNAS\n",
    "    df_selecionado = df_selecionado.withColumnRenamed(\"value.after.condominio_id\", \"condominio_id\") \\\n",
    "                                   .withColumnRenamed(\"value.after.data_registro\", \"data_registro\") \\\n",
    "                                   .withColumnRenamed(\"value.after.morador_id\", \"morador_id\") \\\n",
    "                                   .withColumnRenamed(\"value.after.nome\", \"nome\") \\\n",
    "                                   .withColumnRenamed(\"value.op\", \"op\") \\\n",
    "                                   .withColumnRenamed(\"value.before\", \"before\") \\\n",
    "                                   .withColumnRenamed(\"value.ts_ms\", \"ts_ms\")\n",
    "    \n",
    "    # SELECIONA A ULTIMA ATUALIZAÇÃO DO DADO\n",
    "    df_selecionado.createOrReplaceGlobalTempView('view_moradores')\n",
    "    \n",
    "    cdc_unique = spark.sql('''\n",
    "    WITH \n",
    "        qualify as (select \n",
    "            *, \n",
    "            ROW_NUMBER() over(partition by morador_id order by ts_ms desc) as qualify \n",
    "        from global_temp.view_moradores)\n",
    "    select * from qualify where qualify = 1''')\n",
    "    \n",
    "    # AJUSTA O TIPO DA COLUNA DATA_REGISTRO\n",
    "    cdc_unique = cdc_unique.withColumn(\"data_registro\", to_date(expr(\"date_add('1970-01-01', data_registro)\")))\n",
    "    \n",
    "    # UPSERT\n",
    "    (deltaTable.alias('b')\n",
    "        .merge(cdc_unique.alias('d'), \n",
    "        'b.morador_id = d.morador_id')\n",
    "        .whenMatchedUpdateAll(condition = \"d.op = 'u'\")\n",
    "        .whenMatchedDelete(condition = \"d.op = 'd'\")\n",
    "        .whenNotMatchedInsertAll(condition = \"d.op = 'c' or d.op = 'u'\")\n",
    "        .execute()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "7577f0af-c9d7-4e92-9eda-f7c9f8188029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7889983a60d0>"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_moradores.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "bb7004b0-6b78-4571-836d-1fe6d3161e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-------------+-------------+\n",
      "|morador_id|nome              |condominio_id|data_registro|\n",
      "+----------+------------------+-------------+-------------+\n",
      "|604       |Tiago             |642          |2024-08-01   |\n",
      "|603       |Andre             |643          |2024-08-01   |\n",
      "|602       |Amber Bishop      |682          |2024-05-28   |\n",
      "|601       |Laura Guerrero    |603          |2023-05-24   |\n",
      "|600       |Christopher Petty |659          |2023-12-03   |\n",
      "|599       |Jonathan Page     |683          |2023-07-20   |\n",
      "|598       |Donald Barrett    |673          |2020-08-20   |\n",
      "|597       |Russell Carr      |605          |2024-06-19   |\n",
      "|596       |Michele Dawson    |639          |2023-04-08   |\n",
      "|595       |Nicole Mack       |689          |2021-12-27   |\n",
      "|594       |Angelica Krueger  |674          |2022-06-28   |\n",
      "|593       |Donna Wells       |663          |2023-12-19   |\n",
      "|592       |Katherine Martinez|658          |2024-02-17   |\n",
      "|591       |Elizabeth Cummings|637          |2022-03-07   |\n",
      "|590       |Mr. James Peterson|690          |2022-06-06   |\n",
      "|589       |Alyssa Williams   |636          |2020-06-15   |\n",
      "|588       |John Berger       |618          |2022-02-02   |\n",
      "|587       |David Green       |679          |2020-02-25   |\n",
      "|586       |Craig Rosales     |604          |2020-03-20   |\n",
      "|585       |Kristen Wright    |638          |2020-05-07   |\n",
      "|584       |Kenneth Patterson |670          |2020-01-28   |\n",
      "|583       |Michael Martin    |696          |2023-01-13   |\n",
      "|582       |Brenda Rasmussen  |684          |2021-07-05   |\n",
      "|581       |Catherine Harris  |700          |2023-05-07   |\n",
      "|580       |Lori Dawson       |653          |2022-02-14   |\n",
      "|579       |Samantha Hunt     |674          |2024-02-07   |\n",
      "|578       |Daniel Olson      |636          |2020-12-17   |\n",
      "|577       |Tiffany Stafford  |642          |2022-11-24   |\n",
      "|576       |William Myers     |662          |2020-09-29   |\n",
      "|575       |Amanda Dixon      |699          |2020-07-23   |\n",
      "+----------+------------------+-------------+-------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q('select * from moradores order by 1 desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "c78480d8-a9b4-4349-8a2d-1a3ddfdc109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGREGA CDC CONDOMINIOS\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('value', StructType([\n",
    "        StructField('before', StructType([\n",
    "            StructField('condominio_id', IntegerType(), True), \n",
    "            StructField('nome', StringType(), True), \n",
    "            StructField('endereco', StringType(), True)\n",
    "        ]), True), \n",
    "        StructField('after', \n",
    "            StructType([\n",
    "                StructField('condominio_id', IntegerType(), True), \n",
    "                StructField('nome', StringType(), True), \n",
    "                StructField('endereco', StringType(), True)\n",
    "            ]), True), \n",
    "        StructField('source', \n",
    "                    StructType([\n",
    "                        StructField('version', StringType(), True), \n",
    "                        StructField('connector', StringType(), True), \n",
    "                        StructField('name', StringType(), True), \n",
    "                        StructField('ts_ms', LongType(), True), \n",
    "                        StructField('snapshot', StringType(), True),\n",
    "                        StructField('db', StringType(), True), \n",
    "                        StructField('sequence', StringType(), True), \n",
    "                        StructField('schema', StringType(), True), \n",
    "                        StructField('table', StringType(), True), \n",
    "                        StructField('txId', LongType(), True), \n",
    "                        StructField('lsn', LongType(), True), \n",
    "                        StructField('xmin', LongType(), True)\n",
    "                    ]), True), \n",
    "        StructField('op', StringType(), True), \n",
    "        StructField('ts_ms', LongType(), True), \n",
    "        StructField('transaction', StructType([\n",
    "            StructField('id', StringType(), True), \n",
    "            StructField('total_order', LongType(), True), \n",
    "            StructField('data_collection_order', LongType(), True)\n",
    "        ]), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "df_stream = spark.readStream.format('parquet')\\\n",
    "    .schema(schema).load(f's3a://condomanage/raw/cdc/postgres.public.condominios')\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, 's3a://condomanage/bronze/condominios')\n",
    "\n",
    "stream_condominios = (df_stream.writeStream\\\n",
    "    .option(\"checkpointLocation\", f\"s3a://condomanage/raw/cdc/condominios_checkpoint/\")\\\n",
    "    .foreachBatch(lambda df, batchID: upsert_condominios(df, deltaTable))\n",
    "    .trigger(availableNow=True))\n",
    "    \n",
    "\n",
    "\n",
    "def upsert_condominios(df, deltaTable):\n",
    "    df_selecionado = df.select(\n",
    "        \"value.after.endereco\",\n",
    "        \"value.after.nome\",\n",
    "        when(df.value.op == 'd', df.value.before.condominio_id).otherwise(df.value.after.condominio_id).alias('condominio_id'),\n",
    "        \"value.op\",\n",
    "        \"value.before\",\n",
    "        \"value.ts_ms\"\n",
    "    )\n",
    "    \n",
    "   # RENOMEIA AS COLUNAS\n",
    "    df_selecionado = df_selecionado.withColumnRenamed(\"value.after.endereco\", \"endereco\") \\\n",
    "                                   .withColumnRenamed(\"value.after.nome\", \"nome\") \\\n",
    "                                   .withColumnRenamed(\"value.after.condominio_id\", \"condominio_id\") \\\n",
    "                                   .withColumnRenamed(\"value.op\", \"op\") \\\n",
    "                                   .withColumnRenamed(\"value.before\", \"before\") \\\n",
    "                                   .withColumnRenamed(\"value.ts_ms\", \"ts_ms\")\n",
    "    \n",
    "    # SELECIONA A ULTIMA ATUALIZAÇÃO DO DADO\n",
    "    df_selecionado.createOrReplaceGlobalTempView('view_condominios')\n",
    "    \n",
    "    cdc_unique = spark.sql('''\n",
    "    WITH \n",
    "        qualify as (select \n",
    "            *, \n",
    "            ROW_NUMBER() over(partition by condominio_id order by ts_ms desc) as qualify \n",
    "        from global_temp.view_condominios)\n",
    "    select * from qualify where qualify = 1''')\n",
    "    \n",
    "    # UPSERT\n",
    "    (deltaTable.alias('b')\n",
    "        .merge(cdc_unique.alias('d'), \n",
    "        'b.condominio_id = d.condominio_id')\n",
    "        .whenMatchedUpdateAll(condition = \"d.op = 'u'\")\n",
    "        .whenMatchedDelete(condition = \"d.op = 'd'\")\n",
    "        .whenNotMatchedInsertAll(condition = \"d.op = 'c'\")\n",
    "        .execute()\n",
    "    )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "e2a72cd6-9fbf-4e2f-b7db-a631ab7810e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7889a9a16610>"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad051139-9ac7-4511-8cf8-49ec56fbf8af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IMOVEIS Stream\n",
    "# Definindo o schema do PySpark\n",
    "schema = StructType([\n",
    "StructField('value', StructType([\n",
    "    StructField(\"before\", StructType([\n",
    "        StructField(\"imovel_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"tipo\", StringType(), nullable=False),\n",
    "        StructField(\"condominio_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"valor\", BinaryType(), nullable=False)  # Coluna binária para decimal\n",
    "    ]), nullable=True),\n",
    "    StructField(\"after\", StructType([\n",
    "        StructField(\"imovel_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"tipo\", StringType(), nullable=False),\n",
    "        StructField(\"condominio_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"valor\", BinaryType(), nullable=False)  # Coluna binária para decimal\n",
    "    ]), nullable=True),\n",
    "    StructField(\"source\", StructType([\n",
    "        StructField(\"version\", StringType(), nullable=False),\n",
    "        StructField(\"connector\", StringType(), nullable=False),\n",
    "        StructField(\"name\", StringType(), nullable=False),\n",
    "        StructField(\"ts_ms\", LongType(), nullable=False),\n",
    "        StructField(\"snapshot\", StringType(), nullable=True),\n",
    "        StructField(\"db\", StringType(), nullable=False),\n",
    "        StructField(\"sequence\", StringType(), nullable=True),\n",
    "        StructField(\"schema\", StringType(), nullable=False),\n",
    "        StructField(\"table\", StringType(), nullable=False),\n",
    "        StructField(\"txId\", LongType(), nullable=True),\n",
    "        StructField(\"lsn\", LongType(), nullable=True),\n",
    "        StructField(\"xmin\", LongType(), nullable=True)\n",
    "    ]), nullable=False),\n",
    "    StructField(\"op\", StringType(), nullable=False),\n",
    "    StructField(\"ts_ms\", LongType(), nullable=True),\n",
    "    StructField(\"transaction\", StructType([\n",
    "        StructField(\"id\", StringType(), nullable=False),\n",
    "        StructField(\"total_order\", LongType(), nullable=False),\n",
    "        StructField(\"data_collection_order\", LongType(), nullable=False)\n",
    "    ]), nullable=True)\n",
    "]))\n",
    "])\n",
    "\n",
    "df_stream = spark.readStream.format('parquet')\\\n",
    "    .schema(schema).load(f's3a://condomanage/raw/cdc/postgres.public.imoveis')\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, 's3a://condomanage/bronze/imoveis')\n",
    "\n",
    "stream = (df_stream.writeStream\\\n",
    "    .option(\"checkpointLocation\", f\"s3a://condomanage/raw/cdc/imoveis_checkpoint/\")\\\n",
    "    .foreachBatch(lambda df, batchID: upsert_imoveis(df, deltaTable))\n",
    "    .trigger(availableNow=True))\n",
    "    \n",
    "\n",
    "\n",
    "def upsert_imoveis(df, deltaTable):\n",
    "    df_selecionado = df.select(\n",
    "        \"value.after.tipo\",\n",
    "        \"value.after.valor\",\n",
    "        \"value.after.condominio_id\",\n",
    "        when(df.value.op == 'd', df.value.before.imovel_id).otherwise(df.value.after.imovel_id).alias('imovel_id'),\n",
    "        \"value.op\",\n",
    "        #\"value.before\",\n",
    "        \"value.ts_ms\"\n",
    "    )\n",
    "    \n",
    "    df_selecionado = df_selecionado.withColumnRenamed(\"value.after.tipo\", \"tipo\") \\\n",
    "                                   .withColumnRenamed(\"value.after.valor\", \"valor\") \\\n",
    "                                   .withColumnRenamed(\"value.after.condominio_id\", \"condominio_id\") \\\n",
    "                                    .withColumnRenamed(\"value.after.imovel_id\", \"imovel_id\") \\\n",
    "                                   .withColumnRenamed(\"value.op\", \"op\") \\\n",
    "                                   .withColumnRenamed(\"value.ts_ms\", \"ts_ms\")\n",
    "    df =df_selecionado.toPandas()\n",
    "    \n",
    "    df['valor'] = df.apply(lambda x: bytes_to_decimal(x['valor']), axis=1) \n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"tipo\", StringType(), nullable=False),\n",
    "        StructField(\"valor\", DecimalType(15, 2), nullable=False),\n",
    "        StructField(\"condominio_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"imovel_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"condominio_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"op\", StringType(), nullable=False),\n",
    "        StructField(\"ts_ms\", LongType(), nullable=True)\n",
    "    ])\n",
    "    \n",
    "    df_selecionado = spark.createDataFrame(df)\n",
    "    \n",
    "    # SELECIONA A ULTIMA ATUALIZAÇÃO DO DADO\n",
    "    df_selecionado.createOrReplaceGlobalTempView('view_imoveis')\n",
    "    \n",
    "    cdc_unique = spark.sql('''\n",
    "    WITH \n",
    "        qualify as (select \n",
    "            *, \n",
    "            ROW_NUMBER() over(partition by imovel_id order by ts_ms desc) as qualify \n",
    "        from global_temp.view_imoveis)\n",
    "    select * from qualify where qualify = 1''')\n",
    "    \n",
    "    # UPSERT\n",
    "    (deltaTable.alias('b')\n",
    "        .merge(cdc_unique.alias('d'), \n",
    "        'b.imovel_id = d.imovel_id')\n",
    "        .whenMatchedUpdateAll(condition = \"d.op = 'u'\")\n",
    "        .whenMatchedDelete(condition = \"d.op = 'd'\")\n",
    "        .whenNotMatchedInsertAll(condition = \"d.op = 'c'\")\n",
    "        .execute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "29b7ed73-6199-4743-87bf-d87202cf3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGREGA CDC TRANSACOES\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType, DecimalType, BinaryType\n",
    "import pandas as pd\n",
    "import struct\n",
    "\n",
    "# Função para converter bytes para decimal\n",
    "def bytes_to_decimal(byte_list):\n",
    "    # Converte a lista de bytes para um número inteiro\n",
    "    integer_value = int.from_bytes(bytearray(byte_list), byteorder='big', signed=False)\n",
    "    \n",
    "    # Converte o inteiro para decimal com precisão e escala\n",
    "    precision = 15\n",
    "    scale = 2\n",
    "    decimal_value = integer_value / (10 ** scale)\n",
    "    \n",
    "    # Ajusta o valor para a precisão desejada\n",
    "    return round(decimal_value, scale)\n",
    "\n",
    "# Definindo o schema do PySpark\n",
    "schema = StructType([\n",
    "StructField('value', StructType([\n",
    "    StructField(\"before\", StructType([\n",
    "        StructField(\"transacao_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"imovel_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"morador_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"data_transacao\", IntegerType(), nullable=False),\n",
    "        StructField(\"valor_transacao\", BinaryType(), nullable=False),# Coluna binária para decimal\n",
    "    ]), nullable=True),\n",
    "    StructField(\"after\", StructType([\n",
    "        StructField(\"transacao_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"imovel_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"morador_id\", IntegerType(), nullable=False),\n",
    "        StructField(\"data_transacao\", IntegerType(), nullable=False),\n",
    "        StructField(\"valor_transacao\", BinaryType(), nullable=False),# Coluna binária para decimal\n",
    "    ]), nullable=True),\n",
    "    StructField(\"source\", StructType([\n",
    "        StructField(\"version\", StringType(), nullable=False),\n",
    "        StructField(\"connector\", StringType(), nullable=False),\n",
    "        StructField(\"name\", StringType(), nullable=False),\n",
    "        StructField(\"ts_ms\", LongType(), nullable=False),\n",
    "        StructField(\"snapshot\", StringType(), nullable=True),\n",
    "        StructField(\"db\", StringType(), nullable=False),\n",
    "        StructField(\"sequence\", StringType(), nullable=True),\n",
    "        StructField(\"schema\", StringType(), nullable=False),\n",
    "        StructField(\"table\", StringType(), nullable=False),\n",
    "        StructField(\"txId\", LongType(), nullable=True),\n",
    "        StructField(\"lsn\", LongType(), nullable=True),\n",
    "        StructField(\"xmin\", LongType(), nullable=True)\n",
    "    ]), nullable=False),\n",
    "    StructField(\"op\", StringType(), nullable=False),\n",
    "    StructField(\"ts_ms\", LongType(), nullable=True),\n",
    "    StructField(\"transaction\", StructType([\n",
    "        StructField(\"id\", StringType(), nullable=False),\n",
    "        StructField(\"total_order\", LongType(), nullable=False),\n",
    "        StructField(\"data_collection_order\", LongType(), nullable=False)\n",
    "    ]), nullable=True)\n",
    "]))\n",
    "            ])\n",
    "df = spark.read.format(\"parquet\").schema(schema).load('s3a://condomanage/raw/cdc/postgres.public.transacoes')\n",
    "\n",
    "df_selecionado = df.select(\n",
    "    \"value.after.imovel_id\",\n",
    "    \"value.after.morador_id\",\n",
    "    \"value.after.data_transacao\",\n",
    "    \"value.after.valor_transacao\",\n",
    "    when(df.value.op == 'd', df.value.before.transacao_id).otherwise(df.value.after.transacao_id).alias('transacao_id'),\n",
    "    \"value.op\",\n",
    "    \"value.ts_ms\"\n",
    ")\n",
    "\n",
    "df_selecionado = df_selecionado.withColumnRenamed(\"value.after.imovel_id\", \"imovel_id\") \\\n",
    "                               .withColumnRenamed(\"value.after.morador_id\", \"morador_id\") \\\n",
    "                               .withColumnRenamed(\"value.after.data_transacao\", \"data_transacao\") \\\n",
    "                               .withColumnRenamed(\"value.after.valor_transacao\", \"valor_transacao\") \\\n",
    "                               .withColumnRenamed(\"value.after.transacao_id\", \"transacao_id\") \\\n",
    "                               .withColumnRenamed(\"value.op\", \"op\") \\\n",
    "                               .withColumnRenamed(\"value.ts_ms\", \"ts_ms\")\n",
    "df =df_selecionado.toPandas()\n",
    "\n",
    "df['valor_transacao'] = df.apply(lambda x: bytes_to_decimal(x['valor_transacao']), axis=1) \n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"imovel_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"morador_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"data_transacao\", IntegerType(), nullable=False),\n",
    "    StructField(\"valor_transacao\", DecimalType(15, 2), nullable=False),\n",
    "    StructField(\"transacao_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"op\", StringType(), nullable=False),\n",
    "    StructField(\"ts_ms\", LongType(), nullable=True)\n",
    "])\n",
    "\n",
    "df_selecionado = spark.createDataFrame(df)\n",
    "\n",
    "# SELECIONA A ULTIMA ATUALIZAÇÃO DO DADO\n",
    "df_selecionado.createOrReplaceTempView('delta_t')\n",
    "\n",
    "cdc_unique = spark.sql('''\n",
    "WITH \n",
    "    qualify as (select \n",
    "        imovel_id, morador_id, cast(data_transacao as integer), valor_transacao, transacao_id, op, ts_ms,\n",
    "        ROW_NUMBER() over(partition by transacao_id order by ts_ms desc) as qualify \n",
    "    from delta_t)\n",
    "select * from qualify where qualify = 1''')\n",
    "\n",
    "# AJUSTA O TIPO DA COLUNA DATA_REGISTRO\n",
    "cdc_unique = cdc_unique.withColumn(\"data_transacao\", to_date(expr(\"date_add('1970-01-01', data_transacao)\")))\n",
    "\n",
    "bronze = DeltaTable.forPath(spark, 's3a://condomanage/bronze/transacoes')\n",
    "\n",
    "# UPSERT\n",
    "(bronze.alias('b')\n",
    "    .merge(cdc_unique.alias('d'), \n",
    "    'b.transacao_id = d.transacao_id')\n",
    "    .whenMatchedUpdateAll(condition = \"d.op = 'u'\")\n",
    "    .whenMatchedDelete(condition = \"d.op = 'd'\")\n",
    "    .whenNotMatchedInsertAll(condition = \"d.op = 'c'\")\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "00e60a9c-d778-4794-915f-a38feae8b963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+----------+--------------+---------------+\n",
      "|transacao_id|imovel_id|morador_id|data_transacao|valor_transacao|\n",
      "+------------+---------+----------+--------------+---------------+\n",
      "|550         |502      |566       |2024-05-01    |221267.96      |\n",
      "|597         |502      |597       |2024-06-05    |167102.95      |\n",
      "+------------+---------+----------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q('select * from transacoes where imovel_id = 502')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "02b9a83c-6cac-48d8-9037-7be0a30bad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+----------+--------------+---------------+\n",
      "|transacao_id|imovel_id|morador_id|data_transacao|valor_transacao|\n",
      "+------------+---------+----------+--------------+---------------+\n",
      "|550         |502      |566       |2024-05-01    |1.00           |\n",
      "|597         |502      |597       |2024-06-03    |167102.95      |\n",
      "|601         |502      |566       |2024-08-01    |100000.00      |\n",
      "+------------+---------+----------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q('select * from transacoes where imovel_id = 502')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e18d5a02-a848-4ccf-b818-5eb0ceff2e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q(f\"\"\"\n",
    "            CREATE  TABLE  IF NOT EXISTS teste\n",
    "            USING DELTA\n",
    "            LOCATION 's3a://condomanage/teste'\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "05d1946e-5e9b-41f2-8b23-d15cc2afa928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9152e6b0-210f-4095-96d4-05098e6349c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4fee6b75-48b9-42d1-bcb2-9034c0553909",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_to_decimal_udf = udf(bytes_to_decimal, DecimalType(15, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "4d480d2d-3fa3-438e-9197-dcc7a45f89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "ad13a59e-48ed-483a-8757-fdabbd881a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "f00ab469-de2f-4652-b5c5-2de3edcdaaa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7889a9eb2fd0>"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "4d10cbf6-e139-4f0e-bdbd-d916243f55ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------------------+---------------------------------------------------------+\n",
      "|condominio_id|nome                         |endereco                                                 |\n",
      "+-------------+-----------------------------+---------------------------------------------------------+\n",
      "|620          |Page, Griffin and Vazquez vcc|866 Brandon Underpass Suite 051, Kristinborough, NY 53179|\n",
      "+-------------+-----------------------------+---------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q('select * from condominios where condominio_id = 620')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "21a87847-497a-42c2-a48e-e2cdcd8781a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_df',\n",
       " '_jwrite',\n",
       " '_spark',\n",
       " '_sq',\n",
       " 'foreach',\n",
       " 'foreachBatch',\n",
       " 'format',\n",
       " 'option',\n",
       " 'options',\n",
       " 'outputMode',\n",
       " 'partitionBy',\n",
       " 'queryName',\n",
       " 'start',\n",
       " 'toTable',\n",
       " 'trigger']"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ea4cf-ab39-4e46-a78b-d5a8de4c7fac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
