{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc7ee33-3944-42bf-a632-22c4da8479a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELTA\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\n",
    "\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "        .set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-core_2.12:2.4.0,org.postgresql:postgresql:42.6.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\n",
    "        .set('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension')\n",
    "        .set('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog')\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .set(\"spark.hadoop.fs.s3a..connection.ssl.enabled\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://minioserver:9000\")\n",
    "        .set('spark.hadoop.fs.s3a.access.key', 'SmRXzZ0KrpROvPSKDddy')\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', 'ApanXjLP51CxV3bjPqOlankgLDpeoIZyR4xewaOS')\n",
    ")\n",
    "\n",
    "# Inicializa a sessão Spark com suporte ao Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataIngestion\") \\\n",
    "    .master(\"local\")\\\n",
    "    .config(conf=conf)\\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07e3502-45a6-4c09-9eb2-0fec2e79f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ICEBERG\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\n",
    "\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.3.1,org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.6.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\n",
    "    .set(\"spark.sql.extensions\", \n",
    "         \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") # Use Iceberg with Spark\n",
    "    \n",
    "    .set(\"spark.sql.catalog.data\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .set(\"spark.sql.catalog.data.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .set(\"spark.sql.catalog.data.warehouse\", \"s3a://warehouse\")\n",
    "    .set(\"spark.sql.catalog.data.type\", \"hadoop\")\n",
    "    .set(\"spark.sql.catalog.data.s3.endpoint\", \"http://minioserver:9000\")\n",
    "    .set(\"spark.sql.defaultCatalog\", \"data\") # Name of the Iceberg catalog\n",
    "    .set(\"spark.sql.catalogImplementation\", \"in-memory\")\n",
    "    .set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    .set(\"spark.network.timeout\", \"400000\")\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Disable below line to see INFO logs\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "def load_config(spark_context: SparkContext):\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"SmRXzZ0KrpROvPSKDddy\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"ApanXjLP51CxV3bjPqOlankgLDpeoIZyR4xewaOS\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://minioserver:9000\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"1\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"10000\")\n",
    "\n",
    "load_config(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df476e7-886b-4a61-9d91-4be5c834dcf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c2c6dd81-1630-49db-9dc9-4b436dd6b9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações do PostgreSQL\n",
    "pg_url = \"jdbc:postgresql://postgres:5432/db\"\n",
    "pg_properties = {\n",
    "    \"user\": \"user\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Carrega dados das tabelas PostgreSQL\n",
    "def load_table(table_name):\n",
    "    return spark.read.jdbc(url=pg_url, table=table_name, properties=pg_properties)\n",
    "\n",
    "condominios_df = load_table(\"public.condominios\")\n",
    "moradores_df = load_table(\"public.moradores\")\n",
    "transacoes_df = load_table(\"public.transacoes\")\n",
    "imoveis_df = load_table(\"public.imoveis\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d49b6d99-2354-4343-9369-c6c98e2977f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações do MinIO\n",
    "minio_endpoint = \"http://minioserver:9000\"\n",
    "minio_access_key = \"minioadmin\"\n",
    "minio_secret_key = \"minioadmin\"\n",
    "minio_bucket = \"condomanage\"\n",
    "\n",
    "# Salva os dados no MinIO em formato Parquet\n",
    "def save_to_minio(df, path):\n",
    "    df.write.mode(\"overwrite\").parquet(f\"s3a://{minio_bucket}/raw/{path}\")\n",
    "\n",
    "save_to_minio(condominios, \"condominios\")\n",
    "save_to_minio(moradores, \"moradores\")\n",
    "save_to_minio(imoveis, \"imoveis\")\n",
    "save_to_minio(transacoes, \"transacoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c186d5bb-b2df-471c-b2b9-4e3e9127f806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva os dados no MinIO em formato Delta\n",
    "def save_to_delta(df, path):\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(f\"s3a://{minio_bucket}/raw/delta_{path}\")\n",
    "\n",
    "save_to_delta(condominios_df, \"condominios_delta\")\n",
    "save_to_delta(moradores_df, \"moradores_delta\")\n",
    "save_to_delta(imoveis_df, \"imoveis_delta\")\n",
    "save_to_delta(transacoes_df, \"transacoes_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "737d44c2-5e61-46f4-bb29-c32ba9c760e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva os dados no MinIO em formato iceberg\n",
    "def save_to_iceberg(df, table):\n",
    "    df.writeTo(f\"{table}\").using('iceberg').createOrReplace()\n",
    "\n",
    "save_to_iceberg(condominios_df, \"condominios_iceberg\")\n",
    "save_to_iceberg(moradores_df, \"moradores_iceberg\")\n",
    "save_to_iceberg(imoveis_df, \"imoveis_iceberg\")\n",
    "save_to_iceberg(transacoes_df, \"transacoes_iceberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f410a909-0baf-4311-a017-64d904762403",
   "metadata": {},
   "outputs": [],
   "source": [
    "condominios_df.write.format('delta').save('s3a://condomanage/bronze/upsell/condominios')\n",
    "moradores_df.write.format('delta').save('s3a://condomanage/bronze/upsell/moradores')\n",
    "transacoes_df.write.format('delta').save('s3a://condomanage/bronze/upsell/transacoes')\n",
    "imoveis_df.write.format('delta').save('s3a://condomanage/bronze/upsell/imoveis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "76ab3b4d-837f-48ea-b052-65d7f547e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('s3a://condomanage/raw/postgres.public.moradores')\n",
    "\n",
    "df_selecionado = df.select(\n",
    "    \"value.after.condominio_id\",\n",
    "    \"value.after.data_registro\",\n",
    "    \"value.after.morador_id\",\n",
    "    \"value.after.nome\",\n",
    "    \"value.op\",\n",
    "    \"value.ts_ms\"\n",
    ")\n",
    "\n",
    "# Renomeia as colunas para facilitar o uso, se necessário\n",
    "df_selecionado = df_selecionado.withColumnRenamed(\"value.after.condominio_id\", \"condominio_id\") \\\n",
    "                               .withColumnRenamed(\"value.after.data_registro\", \"data_registro\") \\\n",
    "                               .withColumnRenamed(\"value.after.morador_id\", \"morador_id\") \\\n",
    "                               .withColumnRenamed(\"value.after.nome\", \"nome\") \\\n",
    "                               .withColumnRenamed(\"value.op\", \"op\") \\\n",
    "                               .withColumnRenamed(\"value.ts_ms\", \"ts_ms\")\n",
    "\n",
    "from pyspark.sql.functions import col, expr, to_date\n",
    "df_selecionado = df_selecionado.withColumn(\"data_registro\", to_date(expr(\"date_add('1970-01-01', data_registro)\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c7b2a7f4-6b52-4030-a79c-96e8e7d7587d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('condominio_id', 'int'),\n",
       " ('data_registro', 'date'),\n",
       " ('morador_id', 'int'),\n",
       " ('nome', 'string'),\n",
       " ('op', 'string'),\n",
       " ('ts_ms', 'bigint')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selecionado.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "107d1e3a-19a5-4074-93f6-edc40e00b4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+----------+-----------------+---+-------------+----------+\n",
      "|condominio_id|data_registro|morador_id|             nome| op|        ts_ms|   as_date|\n",
      "+-------------+-------------+----------+-----------------+---+-------------+----------+\n",
      "|           80|        18826|         9|Valerie Garza &&&|  u|1722358655663|2021-07-18|\n",
      "|           80|        18826|         9| Valerie Garza &&|  u|1722358664771|2021-07-18|\n",
      "|           80|        18826|         9|   Valerie Garza |  u|1722356513277|2021-07-18|\n",
      "|           32|        18608|        10|       Lisa Ryan |  u|1722356513279|2020-12-12|\n",
      "|           78|        18492|        17|     Casey Young |  u|1722356513280|2020-08-18|\n",
      "|           80|        18826|         9|    Valerie Garza|  u|1722359294390|2021-07-18|\n",
      "+-------------+-------------+----------+-----------------+---+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "218737d3-1815-430a-ab75-7b84c58b1e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selecionado.createOrReplaceTempView('teste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1ab8f383-1f00-4368-85ff-ea1fe5d37a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdc = spark.sql('''\n",
    "WITH \n",
    "    qualify as (select *, ROW_NUMBER() over(partition by morador_id order by ts_ms desc) as qualify from teste)\n",
    "select * from qualify where qualify = 1''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "11591808-09c1-4971-af14-ad9b76f73c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------+-------------+\n",
      "|morador_id|            nome|condominio_id|data_registro|\n",
      "+----------+----------------+-------------+-------------+\n",
      "|         2|  Kevin Andersen|           48|   2023-03-11|\n",
      "|         3|  Melissa Harmon|           71|   2020-03-14|\n",
      "|         4|Jennifer Montoya|           15|   2021-07-03|\n",
      "|         5|     Dawn Obrien|           17|   2021-07-15|\n",
      "|         8|  Michael Martin|           40|   2022-09-21|\n",
      "|        11|   Cindy Barrett|           61|   2021-01-24|\n",
      "|        12|  Lawrence Wiley|           38|   2021-12-26|\n",
      "|        13|   Marcus Parker|           22|   2024-07-08|\n",
      "|        14|    Justin Avila|           84|   2021-12-08|\n",
      "|        15|  Tiffany Palmer|           20|   2020-12-03|\n",
      "|        16|Stephanie Parker|           15|   2023-01-16|\n",
      "|        18|    Terry Murray|           59|   2022-04-10|\n",
      "|        19|     Gary Parker|           69|   2021-01-31|\n",
      "|        20|Jason Cunningham|           10|   2020-02-16|\n",
      "|        22|     Daniel Rice|           31|   2022-06-08|\n",
      "|        23| Richard Jackson|           48|   2021-06-30|\n",
      "|        25| Michael Simpson|           75|   2022-05-08|\n",
      "|        26|      Karen Cook|           69|   2020-02-05|\n",
      "|        27|    Debra Howell|           40|   2021-07-12|\n",
      "|        28|  Anthony Harris|           52|   2021-11-17|\n",
      "+----------+----------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('s3a://condomanage/bronze/upsell/moradores').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "19bb7a1e-b93f-4e45-abb7-997f58c3d5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('condominio_id', 'int'),\n",
       " ('data_registro', 'date'),\n",
       " ('morador_id', 'int'),\n",
       " ('nome', 'string'),\n",
       " ('op', 'string'),\n",
       " ('ts_ms', 'bigint'),\n",
       " ('qualify', 'int')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "bronze = DeltaTable.forPath(spark, 's3a://condomanage/bronze/upsell/moradores')\n",
    "cdc.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1fb62a3f-097b-4594-ba45-ab323fae2120",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"date_add(1970-01-01, data_registro)\" due to data type mismatch: Parameter 2 requires the (\"INT\" or \"SMALLINT\" or \"TINYINT\") type, however \"data_registro\" has the type \"DATE\".; line 1 pos 0;\n'Project [condominio_id#5913, to_date(date_add(cast(1970-01-01 as date), data_registro#5962), None, Some(Etc/UTC)) AS data_registro#5995, morador_id#5915, nome#5916, op#5917, ts_ms#5918L, qualify#5969]\n+- WithCTE\n   :- CTERelationDef 8, false\n   :  +- SubqueryAlias qualify\n   :     +- Project [condominio_id#5913, data_registro#5962, morador_id#5915, nome#5916, op#5917, ts_ms#5918L, qualify#5969]\n   :        +- Project [condominio_id#5913, data_registro#5962, morador_id#5915, nome#5916, op#5917, ts_ms#5918L, qualify#5969, qualify#5969]\n   :           +- Window [row_number() windowspecdefinition(morador_id#5915, ts_ms#5918L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS qualify#5969], [morador_id#5915], [ts_ms#5918L DESC NULLS LAST]\n   :              +- Project [condominio_id#5913, data_registro#5962, morador_id#5915, nome#5916, op#5917, ts_ms#5918L]\n   :                 +- SubqueryAlias teste\n   :                    +- View (`teste`, [condominio_id#5913,data_registro#5962,morador_id#5915,nome#5916,op#5917,ts_ms#5918L])\n   :                       +- Project [condominio_id#5913, to_date(date_add(cast(1970-01-01 as date), data_registro#5914), None, Some(Etc/UTC)) AS data_registro#5962, morador_id#5915, nome#5916, op#5917, ts_ms#5918L]\n   :                          +- Project [value#5911.after.condominio_id AS condominio_id#5913, value#5911.after.data_registro AS data_registro#5914, value#5911.after.morador_id AS morador_id#5915, value#5911.after.nome AS nome#5916, value#5911.op AS op#5917, value#5911.ts_ms AS ts_ms#5918L]\n   :                             +- Relation [value#5911] parquet\n   +- Project [condominio_id#5913, data_registro#5962, morador_id#5915, nome#5916, op#5917, ts_ms#5918L, qualify#5969]\n      +- Filter (qualify#5969 = 1)\n         +- SubqueryAlias qualify\n            +- CTERelationRef 8, true, [condominio_id#5913, data_registro#5962, morador_id#5915, nome#5916, op#5917, ts_ms#5918L, qualify#5969]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cdc \u001b[38;5;241m=\u001b[39m \u001b[43mcdc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_registro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate_add(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1970-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, data_registro)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m (bronze\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mmerge(cdc\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m), \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb.morador_id = d.morador_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mexecute()\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:4789\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   4784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   4785\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   4786\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4787\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   4788\u001b[0m     )\n\u001b[0;32m-> 4789\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"date_add(1970-01-01, data_registro)\" due to data type mismatch: Parameter 2 requires the (\"INT\" or \"SMALLINT\" or \"TINYINT\") type, however \"data_registro\" has the type \"DATE\".; line 1 pos 0;\n'Project [condominio_id#5913, to_date(date_add(cast(1970-01-01 as date), data_registro#5962), None, Some(Etc/UTC)) AS data_registro#5995, morador_id#5915, nome#5916, op#5917, ts_ms#5918L, qualify#5969]\n+- WithCTE\n   :- CTERelationDef 8, false\n   :  +- SubqueryAlias qualify\n   :     +- Project [condominio_id#5913, data_registro#5962, morador_id#5915, nome#5916, op#5917, ts_ms#5918L, qualify#5969]\n   :        +- Project [condominio_id#5913, data_registro#5962, morador_id#5915, nome#5916, op#5917, ts_ms#5918L, qualify#5969, qualify#5969]\n   :           +- Window [row_number() windowspecdefinition(morador_id#5915, ts_ms#5918L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS qualify#5969], [morador_id#5915], [ts_ms#5918L DESC NULLS LAST]\n   :              +- Project [condominio_id#5913, data_registro#5962, morador_id#5915, nome#5916, op#5917, ts_ms#5918L]\n   :                 +- SubqueryAlias teste\n   :                    +- View (`teste`, [condominio_id#5913,data_registro#5962,morador_id#5915,nome#5916,op#5917,ts_ms#5918L])\n   :                       +- Project [condominio_id#5913, to_date(date_add(cast(1970-01-01 as date), data_registro#5914), None, Some(Etc/UTC)) AS data_registro#5962, morador_id#5915, nome#5916, op#5917, ts_ms#5918L]\n   :                          +- Project [value#5911.after.condominio_id AS condominio_id#5913, value#5911.after.data_registro AS data_registro#5914, value#5911.after.morador_id AS morador_id#5915, value#5911.after.nome AS nome#5916, value#5911.op AS op#5917, value#5911.ts_ms AS ts_ms#5918L]\n   :                             +- Relation [value#5911] parquet\n   +- Project [condominio_id#5913, data_registro#5962, morador_id#5915, nome#5916, op#5917, ts_ms#5918L, qualify#5969]\n      +- Filter (qualify#5969 = 1)\n         +- SubqueryAlias qualify\n            +- CTERelationRef 8, true, [condominio_id#5913, data_registro#5962, morador_id#5915, nome#5916, op#5917, ts_ms#5918L, qualify#5969]\n"
     ]
    }
   ],
   "source": [
    "cdc = cdc.withColumn(\"data_registro\", to_date(expr(\"date_add('1970-01-01', data_registro)\")))\n",
    "\n",
    "(bronze.alias('b')\n",
    "    .merge(cdc.alias('d'), \n",
    "    'b.morador_id = d.morador_id')\n",
    "    .whenMatchedUpdateAll(condition = \"d.op = 'u'\")\n",
    "    .whenMatchedDelete(condition = \"d.op = 'd'\")\n",
    "    .whenNotMatchedInsertAll(condition = \"d.op = 'i'\")\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a13f98fa-dbd2-4791-ab09-597795d0b024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------+-------------+\n",
      "|morador_id|            nome|condominio_id|data_registro|\n",
      "+----------+----------------+-------------+-------------+\n",
      "|         2|  Kevin Andersen|           48|   2023-03-11|\n",
      "|         3|  Melissa Harmon|           71|   2020-03-14|\n",
      "|         4|Jennifer Montoya|           15|   2021-07-03|\n",
      "|         5|     Dawn Obrien|           17|   2021-07-15|\n",
      "|         8|  Michael Martin|           40|   2022-09-21|\n",
      "|        11|   Cindy Barrett|           61|   2021-01-24|\n",
      "|        12|  Lawrence Wiley|           38|   2021-12-26|\n",
      "|        13|   Marcus Parker|           22|   2024-07-08|\n",
      "|        14|    Justin Avila|           84|   2021-12-08|\n",
      "|        15|  Tiffany Palmer|           20|   2020-12-03|\n",
      "|        16|Stephanie Parker|           15|   2023-01-16|\n",
      "|        18|    Terry Murray|           59|   2022-04-10|\n",
      "|        19|     Gary Parker|           69|   2021-01-31|\n",
      "|        20|Jason Cunningham|           10|   2020-02-16|\n",
      "|        22|     Daniel Rice|           31|   2022-06-08|\n",
      "|        23| Richard Jackson|           48|   2021-06-30|\n",
      "|        25| Michael Simpson|           75|   2022-05-08|\n",
      "|        26|      Karen Cook|           69|   2020-02-05|\n",
      "|        27|    Debra Howell|           40|   2021-07-12|\n",
      "|        28|  Anthony Harris|           52|   2021-11-17|\n",
      "+----------+----------------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('s3a://condomanage/bronze/upsell/moradores').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72fc367-caef-4c0c-be21-4faf7f2e278c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
